This page describes proposed syntax of hamake-file 

<wiki:toc max_depth="1" />

= Introduction =
Hamake is based on dataflow programming model - order of execution of tasks is controlled by data and not by tasks themselfs. Contrary to Apache ant or make in hamake-file you describe your data first, then tasks. Task A will be called before task B in case input path of task B intercepts with output path of task A. For a particular task, it will be triggered only if input data has been renewed or validity period of output data has been expired.

= Root Tags Description =

==`<project>`==

This is the root element. All other elements should be whithin this tag. The _project_ has following attributes:
|| *Attribute* || *Description* || *Required* ||
|| name || the name of the project. || No ||
|| default || the default dataflow transformation rule to start with when no start dataflow transformation rules are given as Hamake command-line parameters. || No ||

Each project defines one or more dataflow transformation rules. Each dataflow transformation rule define mapping of your data and task that will process this data. Task can be Hadoop map-reduce job, Pig script or local program (script). When running Hamake, you can select which transformation rule(s) you want to start with. When no transformation rules are given, the project's default is used.

Allowed sub-elements: *`<property>`*, *`<fold>`*, *`<foreach>`*, *`<fileset>`*, *`<file>`*

==`<property>`==

Sets a property (by name and value), in the project. Properties are named values, which can be used in hamake-file. They work like macros. Once defined, they can be used in any string attribute, using the variable syntax ${variable_name}. 

Definition example

{{{
<property name="dfsroot" value="/dfs" />
<property name="numreducers" value="14" />
}}}

Usage example:

{{{
<parameter value="${dfsroot}/data" />
}}}

==`<fileset>`==

A fileset is a group of files. These files can be found in a directory tree starting in a base directory and are matched by mask or be a single file. 

|| *Attribute* || *Description* || *Required* ||
|| id || the ID of the element. || No ||
|| path || the root of the directory tree of this !FileSet. || Yes ||
|| mask ||  filename glob mask. || No ||

==`<file>`==

A file or directory on file system. The actual file system depends on the prefix (e.g. `file:///` results in a file or folder on local FS)
|| *Attribute* || *Description* || *Required* ||
|| id || the ID of the element. || No ||
|| path || path to file system || Yes ||

=Dataflow Transformation Rules=

In Hamake you first define a dataflow transformation rules (DTR) along with data that should be processed, after that you define tasks for a concrete dataflow transformation rule. DTR maps your input data onto output data. Currently you have an option between two mapping types:

  # You can map a group of files at one location to another location(s) with `<foreach>` tag
  # You can reduce a group of file(s) and produce one or more folder(s) or file(s) with `<fold>` tag.

In case of _foreach_ you define one or more mapping functions as an output. In case of _fold_ you can define any amount of files or folders as an output.

==`<foreach>`==

Dataflow transformation rule that defines one-to-many file mapping between input file(s) and one or more output files. This assumes processing only files which are present at source location, but not present or not current at all of destinations. If task input is _foreach_, there will be one output file in all output folders for each file from input folder. Each input file will be processed by task, that is specified whithin _`<foreach>`_ tag, if all of the following conditions are satisfied:
  # Output file is present
  # Output file time stamp is older than input file
  # Output file time stamp is older than any of time stamps of files with same name in all dependent directories 
 
_foreach_ has following sub-elements: *`<input>`*, *`<output>`*, *`<dependency>`*,  *`<mapreduce>`* or *`<pig>`* or *`<exec>`*

Example:

{{{
<foreach name="jar-listings">
	<input>
		<fileset path="${libdir}/lib" mask="*.jar"/>
	</input>
	<output>
		<identity path="${outdir}/jar-listings"/>
	</output>
	<mapreduce main="com.codeminders.hamake.examples.JarListing" jar="${dist}/hamake-examples-1.0.jar">
		<description>description</description>
		<parameter value="${input}"/>
		<parameter value="${output}"/>
	</mapreduce>
</foreach>
}}}

===`<input>`===

This tag defines input dataset of `<foreach>` dataflow transformation rule. As an input you can specify one and only one fileset with `<fileset>` tag

Example:
{{{
<input>
	<fileset id="jar" dir="${outdir}/jar-listings" mask="*.jar" />
</input>
}}}

===`<output>`===

This tag defines output dataset of `<foreach>` dataflow transformation rule. As an output you can specify mapping function(s) that will be applied to each input file. This element has sub elements: *`<composition>`* or *`<identity>`*

Example:
{{{
<output>
	<identity path="${outdir}/jar-listings-filtered"/>
</output>
}}}

===`<composition>`===

This element defines composit output mapping function for `<foreach>` element. Whithin this tag you can specify one or more *`<identity>`* tags

Example:
{{{
<composition>
	<identity path="${outdir}/jar-listings-filtered"/>
	<identity path="${outdir}/jar-listings-bad"/>
</composition>
}}}

===`<identity>`===

Defines identify output mapping function. This function will create file with the same in the folder specified as path attribute.

Example:
{{{
<identity path="${outdir}/jar-listings-filtered"/>
}}}

==`<fold>`==

Dataflow transformation rule that defines many-to-many mapping between input data and output data. `<fold>` considers all input file(s) to be a set, and if any of them is newer than any of destination, input dataset will be processed.
In its simplest form, `fold` have one input and one output. `<fold>` has following sub-elements: *`<input>`*, *`<output>`*, *`<dependency>`*, *`<mapreduce>`* or *`<pig>`* or *`<exec>`* 

Example:
{{{
<fold name="pig_fold">
	<dependency>
		<file path="${libdir}/somefile.txt"/>
	</dependency>
	<input>
		<file path="${outdir}/class-size-histogram"/>
	</input>
	<output>
		<file path="${outdir}/class-size-median-bin"/>
	</output>
	<pig script="${scripts}/median.pig">
		<parameter value="${input}"/>
		<parameter value="${output}"/>
	</pig>
</fold>
}}}

===`<input>`===

This tag defines input dataset of `<fold>` element. As an input you can specify one or more `<file>` tag(s)

Example:
{{{
<input>
	<file path="${outdir}/class-size-histogram"/>
</input>
}}}

===`<output>`===

This tag defines output dataset for `<foreach>` element. Whithin this tag you can specify one or more file(s) or folder(s) with `<file>` tag

Example:
{{{
<output>
	<file path="${outdir}/class-size-histogram"/>
	<file path="${outdir}/logs/histogram.log"/>
</output>
}}}

= Tasks =

Task defines job to be done. Sequence of the task and the number of times it will be called is determined by dataflow transformation rule. In case you want to specify external data a task will depends on, `<dependency>` tag can be used. Currently there are three types of tasks: hadoop job (_hadoop_), 
pig script (_pig_), script or program on machine, where hamake is running (_exec_). Order of arguments that will be passed to Hadoop job, pig script or exec task are determined by parameters list in the body of the task. 

==`<mapreduce>`==

Launches Hadoop Map Reduce job on the !JobTracker that is determined by environment of the running VM (Hamake). This tag has following attributes
|| *Attribute* || *Description* || *Required* ||
|| name || the name of the task. Used for reporting and can be specified as build target || Yes ||
|| jar || path to jar file that will be run || Yes ||
|| disabled || whether this task is disabled. Disabled tasks are ignored. Allowed values are 'yes','no','true','false'.  || No ||
|| main || the full name of the main class || Yes ||

_mapreduce_ can have following sub-elements: *`<parameter>`*, *`<description>`*

Example: 

{{{
<mapreduce jar="${dist}/hamake-examples-1.0.jar" main="com.codeminders.hamake.examples.ClassSizeHistogram">
	<parameter value="${input}"/>
	<parameter value="${output}"/>
</mapreduce>
}}}

==`<pig>`==

Launches Hadoop Pig script on the !JobTracker that is determined by environment of the running VM (Hamake). _pig_ has following attributes
|| *Attribute* || *Description* || *Required* ||
|| name || the name of the task. Used for reporting and can be specified as build target || Yes ||
|| script || path to script file that will be run || Yes ||
|| disabled || whether this task is disabled. Disabled tasks are ignored. Allowed values are 'yes','no','true','false'.  || No ||

_pig_ can have following sub-elements: *`<parameter>`*, *`<description>`*

Example: 

{{{
<pig script="${scripts}/median.pig">
	<parameter value="${input}"/>
	<parameter value="${output}"/>
</pig>
}}}

==`<exec>`==

Executes script or program locally within the running VM (Hamake). It has following attributes
|| *Attribute* || *Description* || *Required* ||
|| name || the name of the task. Used for reporting and can be specified as build target || Yes ||
|| binary || path to script file or program that will be run || Yes ||
|| disabled || whether this task is disabled. Disabled tasks are ignored. Allowed values are 'yes','no','true','false'.  || No ||

_exec_ can have following sub-elements: *`<parameter>`*, *`<description>`*
Example: 

{{{
<exec name="createFile" binary="/usr/bin/touch">
   <parameter value="/tmp/x0"/>
</exec>
}}}

= Task Parameters =

Arguments are passed to Hadoop Map Reduce job, Pig script or program according to task parameters. Also in parameters you can define task input and output data. Please remember that the order of parameters in task body is important.

==`<parameter>`==

This tag has one attribute _value_ where you can specify any parameter you want as String value. Also you can pass here input and output of the task by specifying its name as variable

Example:
{{{
<parameter value="${libdir}/somefile.txt" />
<parameter value="${histogramDir}" />
}}}

= Task Dependencies =

==`<dependency>`==

Task dependencies are build implicitly by the system itself, based on input and output data of dataflow transformation rule. In case you want to specify some external data a dataflow transformation rule will depend on, you can use `<dependency>` tag whithin `<foreach>` or `<fold>` elements. Tag has one sub-element *`<file>`*

Example:
{{{
<dependency>
	<file id="${libdir}/somefile.txt" />
</dependency>
}}}

= Hamakefile Complite Example =
{{{
<project name="test" default="jar-listings">
	<property name="outdir" value="build/test"/>
	<property name="lib" value="lib"/>
	<property name="dist" value="dist"/>
	<property name="scripts" value="src/scripts"/>
	<foreach name="jar-listings">
		<input>
			<fileset path="${libdir}/lib" mask="*.jar"/>
		</input>
		<output>
			<identity path="${outdir}/jar-listings"/>
		</output>
		<mapreduce main="com.codeminders.hamake.examples.JarListing" jar="${dist}/hamake-examples-1.0.jar">
			<description>description</description>
			<parameter value="${input}"/>
			<parameter value="${output}"/>
		</mapreduce>
	</foreach>
	<foreach name="filter-listing">
		<input>
			<fileset path="${outdir}/jar-listings" mask="*.jar"/>
		</input>
		<output>
			<composition>
				<identity path="${outdir}/jar-listings-filtered"/>
				<identity path="${outdir}/jar-listings-bad"/>
			</composition>
		</output>
		<mapreduce jar="${dist}/hamake-examples-1.0.jar" main="com.codeminders.hamake.examples.JarListingFilter">
			<parameter value="${input}"/>
			<parameter value="${output}"/>
		</mapreduce>
	</foreach>
	<fold name="histogram">
		<input>
			<fileset path="${outdir}/jar-listings-bad"/>
			<fileset path="${outdir}/jar-listings-filtered"/>
		</input>
		<output>
			<file path="${outdir}/class-size-histogram"/>
			<file path="${outdir}/logs/histogram.log"/>
		</output>
		<mapreduce jar="${dist}/hamake-examples-1.0.jar" main="com.codeminders.hamake.examples.ClassSizeHistogram">
			<parameter value="${input}"/>
			<parameter value="${output}"/>
		</mapreduce>
	</fold>
	<fold name="pig_fold">
		<dependency>
			<file path="${libdir}/somefile.txt"/>
		</dependency>
		<input>
			<file path="${outdir}/class-size-histogram"/>
		</input>
		<output>
			<file path="${outdir}/class-size-median-bin"/>
		</output>
		<pig script="${scripts}/median.pig">
			<parameter value="${input}"/>
			<parameter value="${output}"/>
		</pig>
	</fold>
</project>

}}}