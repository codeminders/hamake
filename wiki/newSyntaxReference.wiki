This page describes proposed syntax of hamake-file 

<wiki:toc max_depth="1" />

= Introduction =
Hamake is a lightweight utility and workflow engine for Hadoop. Hamake helps to organize your Hadoop Map Reduce jobs, Pig script and local programs in a workflow and launch them based on dataflow principles - your tasks will be executed as soon as new data will be available for them. Contrary to Apache Ant or Make utilities, hamake-file declare data sources and destinations for each task and only source data state influences the moment of task launch. Task A will be called before task B in case input path of task B intercepts with output path of task A. For a particular task, it will be triggered only if input data has been renewed or validity period of output data has been expired.

= Root Tags Description =

==`<project>`==

This is the root element. All other elements should be whithin this tag. The _project_ has following attributes:
|| *Attribute* || *Description* || *Required* ||
|| name || the name of the project. || No ||
|| default || the default dataflow transformation rule to start with when no start dataflow transformation rules are given as Hamake command-line parameters. || No ||

Each project defines one or more dataflow transformation rules. Each dataflow transformation rule defines two things:
 # mapping of your input data on output data, that make possible to build data dependency graph
 # the way of input data processing: a) process each input item individually; b) process whole set of input items in conjunction.
There are three kind of tasks (processing) available: Hadoop map-reduce job, Pig script and local program (script). When running Hamake, you can select which transformation rule(s) you want to start with. When no transformation rules are given, the project's default is used.

Allowed sub-elements: *`<property>`*, *`<fold>`*, *`<foreach>`*, *`<fileset>`*, *`<file>`*

==`<property>`==

Sets a property (by name and value), in the project. Properties are named values, which can be used in hamake-file. They work like macros. Once defined, they can be used in any string attribute, using the variable syntax ${variable_name}. 

Definition example

{{{
<property name="dfsroot" value="/dfs" />
<property name="numreducers" value="14" />
}}}

Usage example:

{{{
<parameter value="${dfsroot}/data" />
}}}

==`<fileset>`==

A fileset is a group of files. These files can be found in specified directory and are matched by mask or be a single file. If mask is omitted, all the files at specified directory are fit.

|| *Attribute* || *Description* || *Required* ||
|| id || the ID of the element. || No ||
|| path || the root of the directory tree of this !FileSet. || Yes ||
|| mask ||  filename glob mask. || No ||

==`<file>`==

A file or directory on file system. The actual file system depends on the path schema (e.g. `file:///` results in a file or folder on local FS)
|| *Attribute* || *Description* || *Required* ||
|| id || the ID of the element. || No ||
|| path || path to file system || Yes ||

=Dataflow Transformation Rules=

In Hamake you first define a dataflow transformation rules (DTR) along with data that should be processed, after that you define tasks for a concrete dataflow transformation rule. DTR maps your input data onto output data. Currently you have an option between two mapping types:

  # `<foreach>` - one-by-one get files from input, execute specified task for each input file and save result file to appropriate output location
  # `<fold>` - get whole set of input files, execute specified task for this set, save result file(s) to output location(s)

Understanding DTR: 
 * _foreach_ DTR is like a loop over file set from task execution point of view, or one-to-one mapping from data point of view
 * _foreach_ DTR generates the same number of elements on output as it was on input
 * _foreach_ DTR allows to specify input data set from different locations (folders on FS). Output data, as well, can be stored in a set of locations. But it is assumed that there is a way to map each input item on appropriate output item.
 * _fold_ DTR similar to JOIN operation over input data set
 * _fold_ DTR can produce one or more output files

==`<foreach>`==

Dataflow transformation rule that defines one-to-many file mapping between input file(s) and one or more output files. This assumes processing only files which are present at source location, but not present or not current at all of destinations. If task input is _foreach_, there will be one output file in all output folders for each file from input folder. Each input file will be processed by task, that is specified whithin _`<foreach>`_ tag, if all of the following conditions are satisfied:
  # Output file is present
  # Output file time stamp is older than input file
  # Output file time stamp is older than any of time stamps of files with same name in all dependent directories 
 
_foreach_ has following sub-elements: *`<input>`*, *`<output>`*, *`<dependency>`*,  *`<mapreduce>`* or *`<pig>`* or *`<exec>`*

Example:

{{{
<foreach name="jar-listings">
	<input>
		<fileset path="${libdir}/lib" mask="*.jar"/>
	</input>
	<output>
		<identity path="${outdir}/jar-listings"/>
	</output>
	<mapreduce main="com.codeminders.hamake.examples.JarListing" jar="${dist}/hamake-examples-1.0.jar">
		<description>description</description>
		<parameter value="${input}"/>
		<parameter value="${output}"/>
	</mapreduce>
</foreach>
}}}

===`<input>`===

This tag defines input dataset of `<foreach>` dataflow transformation rule. As an input you can specify one and only one fileset with `<fileset>` tag

Example:
{{{
<input>
	<fileset id="jar" dir="${outdir}/jar-listings" mask="*.jar" />
</input>
}}}

===`<output>`===

This tag defines output dataset of `<foreach>` dataflow transformation rule. As an output you can specify mapping function(s) that will be applied to each input file. This element has sub elements: *`<composition>`* or *`<identity>`*

If your data was not changed a long time, but you need to execute some task for sure even for unmodified data, there is {{{expiration}}} attribute that specifies data consistency period. When this period expires, the Hamake would consider input as modified.
Expiration period is defined as: `<value><measure unit>`, <br>
where: value is an integer value and measure unit is one of
 * s - seconds
 * m - minutes
 * h - hours
 * d - days
 * w - weeks

|| *Attribute* || *Description* || *Required* ||
|| expiration || input/output data consistency timeout || No ||

Example:
{{{
<output expiration="1w">
	<identity path="${outdir}/jar-listings-filtered"/>
</output>
}}}

===`<composition>`===

This element defines composit output mapping function for `<foreach>` element. Whithin this tag you can specify one or more *`<identity>`* tags

Example:
{{{
<composition>
	<identity path="${outdir}/jar-listings-filtered"/>
	<identity path="${outdir}/jar-listings-bad"/>
</composition>
}}}

===`<identity>`===

Defines identify output mapping function. This function will create file with the same in the folder specified as path attribute.

Example:
{{{
<identity path="${outdir}/jar-listings-filtered"/>
}}}

==`<fold>`==

Dataflow transformation rule that defines many-to-many mapping between input data and output data. `<fold>` considers all input file(s) to be a set, and if any of them is newer than any of destination, input dataset will be processed.
In its simplest form, `fold` have one input and one output. `<fold>` has following sub-elements: *`<input>`*, *`<output>`*, *`<dependency>`*, *`<mapreduce>`* or *`<pig>`* or *`<exec>`* 

Example:
{{{
<fold name="pig_fold">
	<dependency>
		<file path="${libdir}/somefile.txt"/>
	</dependency>
	<input>
		<file path="${outdir}/class-size-histogram"/>
	</input>
	<output>
		<file path="${outdir}/class-size-median-bin"/>
	</output>
	<pig script="${scripts}/median.pig">
		<parameter value="${input}"/>
		<parameter value="${output}"/>
	</pig>
</fold>
}}}

===`<input>`===

This tag defines input dataset of `<fold>` element. As an input you can specify one or more `<file>` tag(s)

Example:
{{{
<input>
	<file path="${outdir}/class-size-histogram"/>
</input>
}}}

===`<output>`===

This tag defines output dataset for `<fold>` element. Whithin this tag you can specify one or more file(s) or folder(s) with `<file>` tag

Example:
{{{
<output>
	<file path="${outdir}/class-size-histogram"/>
	<file path="${outdir}/logs/histogram.log"/>
</output>
}}}

= Tasks =

Task defines job to be done. Sequence of the task and the number of times it will be called is determined by dataflow transformation rule. In case you want to specify external data a task will depends on, `<dependency>` tag can be used. Currently there are three types of tasks: hadoop job (_hadoop_), 
pig script (_pig_), script or program on machine, where hamake is running (_exec_). Order of arguments that will be passed to Hadoop job, pig script or exec task are determined by parameters list in the body of the task. 

==`<mapreduce>`==

Launches Hadoop Map Reduce job on the !JobTracker that is determined by environment of the running VM (Hamake). This tag has following attributes
|| *Attribute* || *Description* || *Required* ||
|| name || the name of the task. Used for reporting and can be specified as build target || Yes ||
|| jar || path to jar file that will be run || Yes ||
|| disabled || whether this task is disabled. Disabled tasks are ignored. Allowed values are 'yes','no','true','false'.  || No ||
|| main || the full name of the main class || Yes ||

_mapreduce_ can have following sub-elements: *`<parameter>`*, *`<description>`*

Example: 

{{{
<mapreduce jar="${dist}/hamake-examples-1.0.jar" main="com.codeminders.hamake.examples.ClassSizeHistogram">
	<parameter value="${input}"/>
	<parameter value="${output}"/>
</mapreduce>
}}}

==`<pig>`==

Launches Hadoop Pig script on the !JobTracker that is determined by environment of the running VM (Hamake). _pig_ has following attributes
|| *Attribute* || *Description* || *Required* ||
|| name || the name of the task. Used for reporting and can be specified as build target || Yes ||
|| script || path to script file that will be run || Yes ||
|| disabled || whether this task is disabled. Disabled tasks are ignored. Allowed values are 'yes','no','true','false'.  || No ||

_pig_ can have following sub-elements: *`<parameter>`*, *`<description>`*

Example: 

{{{
<pig script="${scripts}/median.pig">
	<parameter value="${input}"/>
	<parameter value="${output}"/>
</pig>
}}}

==`<exec>`==

Executes script or program locally within the running VM (Hamake). It has following attributes
|| *Attribute* || *Description* || *Required* ||
|| name || the name of the task. Used for reporting and can be specified as build target || Yes ||
|| binary || path to script file or program that will be run || Yes ||
|| disabled || whether this task is disabled. Disabled tasks are ignored. Allowed values are 'yes','no','true','false'.  || No ||

_exec_ can have following sub-elements: *`<parameter>`*, *`<description>`*
Example: 

{{{
<exec name="createFile" binary="/usr/bin/touch">
   <parameter value="/tmp/x0"/>
</exec>
}}}

= Task Parameters =

Arguments are passed to Hadoop Map Reduce job, Pig script or program according to task parameters. Also in parameters you can define task input and output data. Please remember that the order of parameters in task body is important.

==`<parameter>`==

This tag has one attribute _value_ where you can specify any parameter you want as String value. Also you can pass here input and output of the task by specifying its name as variable

Example:
{{{
<parameter value="${libdir}/somefile.txt" />
<parameter value="${histogramDir}" />
}}}

= Task Dependencies =

==`<dependency>`==

Task dependencies are build implicitly by the system itself, based on input and output data of dataflow transformation rule. In case you want to specify some external data a dataflow transformation rule will depend on, you can use `<dependency>` tag whithin `<foreach>` or `<fold>` elements. Tag has one sub-element *`<file>`*

Example:
{{{
<dependency>
	<file id="${libdir}/somefile.txt" />
</dependency>
}}}

= Hamakefile Complite Example =
{{{
<project name="test" default="jar-listings">
	<property name="outdir" value="build/test"/>
	<property name="lib" value="lib"/>
	<property name="dist" value="dist"/>
	<property name="scripts" value="src/scripts"/>
	<foreach name="jar-listings">
		<input>
			<fileset path="${libdir}/lib" mask="*.jar"/>
		</input>
		<output>
			<identity path="${outdir}/jar-listings"/>
		</output>
		<mapreduce main="com.codeminders.hamake.examples.JarListing" jar="${dist}/hamake-examples-1.0.jar">
			<description>description</description>
			<parameter value="${input}"/>
			<parameter value="${output}"/>
		</mapreduce>
	</foreach>
	<foreach name="filter-listing">
		<input>
			<fileset path="${outdir}/jar-listings" mask="*.jar"/>
		</input>
		<output>
			<composition>
				<identity path="${outdir}/jar-listings-filtered"/>
				<identity path="${outdir}/jar-listings-bad"/>
			</composition>
		</output>
		<mapreduce jar="${dist}/hamake-examples-1.0.jar" main="com.codeminders.hamake.examples.JarListingFilter">
			<parameter value="${input}"/>
			<parameter value="${output}"/>
		</mapreduce>
	</foreach>
	<fold name="histogram">
		<input>
			<fileset path="${outdir}/jar-listings-bad"/>
			<fileset path="${outdir}/jar-listings-filtered"/>
		</input>
		<output>
			<file path="${outdir}/class-size-histogram"/>
			<file path="${outdir}/logs/histogram.log"/>
		</output>
		<mapreduce jar="${dist}/hamake-examples-1.0.jar" main="com.codeminders.hamake.examples.ClassSizeHistogram">
			<parameter value="${input}"/>
			<parameter value="${output}"/>
		</mapreduce>
	</fold>
	<fold name="pig_fold">
		<dependency>
			<file path="${libdir}/somefile.txt"/>
		</dependency>
		<input>
			<file path="${outdir}/class-size-histogram"/>
		</input>
		<output>
			<file path="${outdir}/class-size-median-bin"/>
		</output>
		<pig script="${scripts}/median.pig">
			<parameter value="${input}"/>
			<parameter value="${output}"/>
		</pig>
	</fold>
</project>

}}}