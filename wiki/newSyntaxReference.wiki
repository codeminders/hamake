This page describes proposed syntax of hamake-file 

<wiki:toc max_depth="1" />

= Introduction =
Hamake is a lightweight workflow engine for Hadoop. Hamake helps to organize your Hadoop Map Reduce jobs, Pig script and local programs and launch them based on dataflow principles - your tasks will be executed as soon as new data will be availible for them. Contrary to Apache Ant or Make utilities, in hamake-file you declare your data sources and destinations first. Tasks are bound to your data. Task A will be called before task B in case input path of task B intercepts with output path of task A. For a particular task, it will be triggered only if input data has been renewed or validity period of output data has been expired.

= Root Tags Description =

==`<project>`==

This is the root element. All other elements should be whithin this tag. The _project_ has following attributes:
|| *Attribute* || *Description* || *Required* ||
|| name || the name of the project. || Yes ||
|| default || the default dataflow transformation rule to start with when no transformation rules are given as Hamake command-line arguments. || No ||

Each project defines one or more dataflow transformation rules. Each transformation rule defines two things:
 # mapping of your input data onto output data, thus you define data dependency graph
 # the way your input data is processed: a) process each input file individually; b) process a set of input items in conjunction. 

When running Hamake, you can select which transformation rule(s) you want to start with. When no transformation rules are given, the project's default is used.

Allowed sub-elements: *`<property>`*, *`<fold>`*, *`<foreach>`*, *`<fileset>`*, *`<file>`*

==`<property>`==

Sets a property (by name and value), in the project. Properties are named values, which can be used in hamake-file. They work like macros. Once defined, they can be used in any string attribute, using the Apache Ant variable syntax ${variable_name}. 

Definition example

{{{
<property name="dfsroot" value="/dfs" />
<property name="numreducers" value="14" />
}}}

Usage example:

{{{
<parameter value="${dfsroot}/data" />
}}}

=Dataflow Transformation Rules=

In Hamake you first define a dataflow transformation rules (DTR) along with data that should be processed, after that you define tasks for a concrete dataflow transformation rule. DTR maps your input data onto output data. Currently you have an option between two mapping types:

  # `<foreach>` - get files one-by-one from input, execute specified task for each input file and save result file to appropriate output location
  # `<fold>` - get a set of input files as a whole, execute specified task for the set, save resulted file(s) to output location(s)

Understanding DTR: 
 * _foreach_ DTR is like one-to-one mapping, from data point of view
 * _foreach_ DTR specify mapping of input files onto each output location
 * _foreach_ DTR allows to combine input files from different locations
 * _fold_ DTR is similar to JOIN operation over input data set
 * _fold_ DTR allows to combine input data sets from different locations
 * _fold_ DTR can produce one or more output items for an input data set

==`<foreach>`==

Dataflow transformation rule which maps a group of files at one location(s) to another location(s). This DTR assumes 1 to 1 file mapping between locations, and can process them incrementally, converting only files which are present at source location(s), but not present or not current at all of destinations. Each input file will produce one output file per output location. Output location file considered to be current if all of the following conditions are satisfied:
   #. Output file is present
   #. Output file time stamp is older than input file
   #. Output file time stamp is older than any of time stamps of files with same name in all dependent directories 
 
_foreach_ has following sub-elements: *`<input>`*, *`<output>`*, *`<dependency>`*,  *`<mapreduce>`* or *`<pig>`* or *`<exec>`*

Example:

{{{
<foreach name="jar-listings">
	<input>
		<fileset path="${libdir}/lib" mask="*.jar"/>
	</input>
	<output>
		<identity path="${outdir}/jar-listings"/>
	</output>
	<mapreduce main="com.codeminders.hamake.examples.JarListing" jar="${dist}/hamake-examples-1.0.jar">
		<description>description</description>
		<parameter value="${input}"/>
		<parameter value="${output}"/>
	</mapreduce>
</foreach>
}}}

===`<input>`===

This tag defines input dataset. 
_foreach_ DTR allows to specify here any number of input filesets.

Example:
{{{
<input>
    <fileset id="jar" dir="${outdir}/jar-listings" mask="*.jar" />
    <fileset id="zip" dir="${outdir}/jar-listings" mask="*.zip" />
</input>
}}}

===`<output>`===

This tag defines output dataset.
_foreach_ DTR allows to specify here mapping function(s) only. This function will be applied to each input file and produce mapped file on output location. At the moment, there are only two mapping functions: 
 * identity - produce file with same name in different location (folder)
 * composition - complex output to several locations (see tag composition description)
`<output>` has a single optional attribute - _validity_period_. It specifies for now many seconds this output considered valid (optional). In other words, this is maximum allowed time difference between inputs and outputs. The period is a number followed by optional letter. Following letters are understood:
 * s - seconds
 * m - minutes
 * h - hours
 * d - days
 * w - weeks

This element has sub elements: *`<composition>`* or *`<identity>`*

Example:
{{{
<output validity="1m">
    <identity path="${outdir}/jar-listings-filtered"/>
</output>
}}}

===`<identity>`===

Defines identity output mapping function. This function will create file with the same in the folder specified as path attribute.

Example:
{{{
<identity path="${outdir}/jar-listings-filtered"/>
}}}

===`<composition>`===

This element allows to define multiple mapping functiona of _foreach_ DTR. Within this tag you can specify one or more *`<identity>`* tags

Example:
{{{
<composition>
    <identity path="${outdir}/jar-listings-filtered"/>
    <identity path="${outdir}/jar-listings-bad"/>
</composition>
}}}

==`<fold>`==

Dataflow transformation rule that defines many-to-many mapping between input data and output data. `<fold>` considers all input file(s) to be a set, and if any of them is newer than any of destination, input dataset will be processed.
In its simplest form, `fold` have one input and one output. `<fold>` has following sub-elements: *`<input>`*, *`<output>`*, *`<dependency>`*, *`<mapreduce>`* or *`<pig>`* or *`<exec>`* 

Example:
{{{
<fold name="pig_fold">
	<dependency>
		<file path="${libdir}/somefile.txt"/>
	</dependency>
	<input>
		<file path="${outdir}/class-size-histogram"/>
	</input>
	<output>
		<file path="${outdir}/class-size-median-bin"/>
	</output>
	<pig script="${scripts}/median.pig">
		<parameter value="${input}"/>
		<parameter value="${output}"/>
	</pig>
</fold>
}}}

===`<input>`===

This tag defines input dataset. 
_fold_ DTR allows to specify here one or more folders or files. It has only one sub-element: `<file>`.

Example:
{{{
<input>
    <file path="${outdir}/class-size-histogram"/>
</input>
}}}

===`<output>`===

This tag defines output dataset. 
_fold_ DTR allows to specify here one or more files or folders. This tag has a single optional attribute - _validity_period_. It specifies for now many seconds this output considered valid (optional). In other words, this is maximum allowed time difference between inputs and outputs. The period is a number followed by optional letter. Following letters are understood:
 * s - seconds
 * m - minutes
 * h - hours
 * d - days
 * w - weeks

Example:
{{{
<output>
    <file path="${outdir}/class-size-histogram"/>
    <file path="${outdir}/logs/histogram.log"/>
</output>
}}}

= Hamake Path Handling =

To specify path in Hamake you can use two tags: `<file>` and `<fileset>`

With `<file>` you can specify a file or a directory on FS. With `<fileset>` you can specify a group of files.

==`<fileset>`==

A fileset is a group of files. In case mask has been  files can be found in specified directory and are matched by mask or be a single file. If mask is omitted, all the files at specified directory are fit.

|| *Attribute* || *Description* || *Required* ||
|| id || the ID of the element. || No ||
|| path || the root directory of this !FileSet. || Yes ||
|| mask ||  filename glob mask. || No ||
|| generation || "generation" of files in fileset. The generation mechanism is described in more depth in Hamake manual.|| No ||

==`<file>`==

A file or directory on file system. The actual file system depends on the path schema (e.g. `file:///` results in a file or folder on local FS).

|| *Attribute* || *Description* || *Required* ||
|| id || the ID of the element. || No ||
|| path || path to file system || Yes ||
|| mask ||  filename glob mask. || No ||
|| variant || how path will be passed as a task argument. Can be _list_, _path_, _mask_ || no ||
|| generation || "generation" of files in fileset. The generation mechanism is described in more depth in Hamake manual. || No ||

In `<file>` you can optionally control how Hamake will pass path as an argument to a task:

 * _list_ - unfold path and path all files from the folder. In case you have specified file in _path_ attribute, only that file will be passed
 * _path_ - pass folder or file as it is
 * _mask_ - pass file with mask. In case there is no _mask_ attribute, no mask will be joined

In case there is no _variant_ attribute, _path_ is default.

= Tasks =

In Hamake there are three kind of tasks available: Hadoop map-reduce job, Pig script and local program (script). Task defines job to be done. Sequence of the task and the number of times it will be called is determined by dataflow transformation rule. In case you want to specify external data a task will depends on, `<dependency>` tag can be used. Currently there are three types of tasks: hadoop job (_hadoop_), 
pig script (_pig_), script or program on machine, where hamake is running (_exec_). Order of arguments that will be passed to Hadoop job, pig script or exec task are determined by order of parameters list in the body of the task. 

==`<mapreduce>`==

Launches Hadoop Map Reduce job on the !JobTracker that is determined by environment of the running VM (Hamake). This tag has following attributes
|| *Attribute* || *Description* || *Required* ||
|| name || the name of the task. Used for reporting and can be specified as build target || Yes ||
|| jar || path to jar file that will be run || Yes ||
|| disabled || whether this task is disabled. Disabled tasks are ignored. Allowed values are 'yes','no','true','false'.  || No ||
|| main || the full name of the main class || Yes ||

_mapreduce_ can have following sub-elements: *`<parameter>`*, *`<description>`*

Example: 

{{{
<mapreduce jar="${dist}/hamake-examples-1.0.jar" main="com.codeminders.hamake.examples.ClassSizeHistogram">
	<parameter value="${input}"/>
	<parameter value="${output}"/>
</mapreduce>
}}}

==`<pig>`==

Launches Hadoop Pig script on the !JobTracker that is determined by environment of the running VM (Hamake). _pig_ has following attributes
|| *Attribute* || *Description* || *Required* ||
|| name || the name of the task. Used for reporting and can be specified as build target || Yes ||
|| script || path to script file that will be run || Yes ||
|| disabled || whether this task is disabled. Disabled tasks are ignored. Allowed values are 'yes','no','true','false'.  || No ||

_pig_ can have following sub-elements: *`<parameter>`*, *`<description>`*

Example: 

{{{
<pig script="${scripts}/median.pig">
	<parameter value="${input}"/>
	<parameter value="${output}"/>
</pig>
}}}

==`<exec>`==

Executes script or program locally within the running VM (Hamake). It has following attributes
|| *Attribute* || *Description* || *Required* ||
|| name || the name of the task. Used for reporting and can be specified as build target || Yes ||
|| binary || path to script file or program that will be run || Yes ||
|| disabled || whether this task is disabled. Disabled tasks are ignored. Allowed values are 'yes','no','true','false'.  || No ||

_exec_ can have following sub-elements: *`<parameter>`*, *`<description>`*
Example: 

{{{
<exec name="createFile" binary="/usr/bin/touch">
   <parameter value="/tmp/x0"/>
</exec>
}}}

= Task Parameters =

Arguments are passed to Hadoop Map Reduce job, Pig script or program according to task parameters. Also in parameters you can define task input and output data. Please remember that the order of parameters in task body is important.

==`<jobconf>`==

With this element you can set Hadoop or Pig JobConf parameter for your task. This element has following attributes:

|| *Attribute* || *Description* || *Required* ||
|| name || name of the JobConf parameter || Yes ||
|| value || value of the JobConf parameter || Yes ||


==`<parameter>`==

This tag has one attribute _value_ where you can specify any parameter you want as String value. Also you can pass here input and output of the task by specifying special built in ${input} and ${output} variables. In case of `<foreach>` a task will recieve a single file  in place of ${input} variable. In case of `<fold>`   

Example:
{{{
<parameter value="${input}" />
<parameter value="${output}" />
<parameter value="${libdir}/somefile.txt" />
<parameter value="${histogramDir}" />
}}}

= Task Dependencies =

==`<dependency>`==

In case you want to specify some external data a dataflow transformation rule will depend on, you can use `<dependency>` tag whithin `<foreach>` or `<fold>` elements. Tag has one sub-element *`<file>`*

Example:
{{{
<dependency>
    <file id="${libdir}/somefile.txt" />
</dependency>
}}}

= Hamakefile Complite Example =
{{{
<project name="test" default="jar-listings">
	<property name="outdir" value="build/test"/>
	<property name="lib" value="lib"/>
	<property name="dist" value="dist"/>
	<property name="scripts" value="src/scripts"/>
	<foreach name="jar-listings">
		<input>
			<fileset path="${libdir}/lib" mask="*.jar"/>
		</input>
		<output>
			<identity path="${outdir}/jar-listings"/>
		</output>
		<mapreduce main="com.codeminders.hamake.examples.JarListing" jar="${dist}/hamake-examples-1.0.jar">
			<description>description</description>
			<parameter value="${input}"/>
			<parameter value="${output}"/>
		</mapreduce>
	</foreach>
	<foreach name="filter-listing">
		<input>
			<fileset path="${outdir}/jar-listings" mask="*.jar"/>
		</input>
		<output>
			<composition>
				<identity path="${outdir}/jar-listings-filtered"/>
				<identity path="${outdir}/jar-listings-bad"/>
			</composition>
		</output>
		<mapreduce jar="${dist}/hamake-examples-1.0.jar" main="com.codeminders.hamake.examples.JarListingFilter">
			<parameter value="${input}"/>
			<parameter value="${output}"/>
		</mapreduce>
	</foreach>
	<fold name="histogram">
		<input>
			<fileset path="${outdir}/jar-listings-bad"/>
			<fileset path="${outdir}/jar-listings-filtered"/>
		</input>
		<output>
			<file path="${outdir}/class-size-histogram"/>
			<file path="${outdir}/logs/histogram.log"/>
		</output>
		<mapreduce jar="${dist}/hamake-examples-1.0.jar" main="com.codeminders.hamake.examples.ClassSizeHistogram">
			<parameter value="${input}"/>
			<parameter value="${output}"/>
		</mapreduce>
	</fold>
	<fold name="pig_fold">
		<dependency>
			<file path="${libdir}/somefile.txt"/>
		</dependency>
		<input>
			<file path="${outdir}/class-size-histogram"/>
		</input>
		<output validity_period="1h">
			<file path="${outdir}/class-size-median-bin"/>
		</output>
		<pig script="${scripts}/median.pig">
			<parameter value="${input}"/>
			<parameter value="${output}"/>
		</pig>
	</fold>
</project>

}}}