version 1.0

=Introduction=

Hamake is a lightweight utility and workflow engine for running Hadoop Map Reduce jobs and Pig scrips on a dataset stored on HDFS. 

==Why Hamake?==

Hadoop jobs are not run alone by themselfs. Usually you require multiple Hadoop jobs to achieve some result. Also sometines you would like to filter your data first and process only files that have changed. Hamake helps you to organize your Hadoop map-reduce jobs or Pig scripts in a workflow, that will be launched and controlled from a single machine. The order of execution of Hadoop map-reduce jobs or Pig scripts in hamake is based on dataflow programming model and is controlled by data, not by tasks themselfs. Hamake is very simple in installation and configuration. In fact all you need to install Hamake is to copy a single jar file to the directory of your choise. 

=Installation=

Hamake runs in Hadoop environment. Please make sure you have installed Hadoop on the machine where you are going to run Hamake and added Hadoop bin directory in PATH environment variable.

{{{
export PATH=$PATH:$HADOOP_HOME/bin
}}}
In contrast to similar software Hamake does not require dedicated server or complex installation. To install Hamake just unpack Hamake distribution archive and copy hamake-j.jar file from the folder where you have unpacked distribution archive to the directory of your choise. Next you should describe workflow definitions in a special-formatted hamake-file and launch the process with _hadoop_ command:   
    Example: {{{hadoop jar hamake-j-1.0.jar path_to_hamake-file}}}

=Using Hamake=

Hamake is a workflow engine. To understand how it works here are simple steps:
 # You define your tasks in a special XML-formatted file (hamake-file) and launch Hamake as Hadoop job giving hamake-file as an argument.
 # Hamake arranges tasks from hamake-file in a direct acyclic graph
 # Tasks are launched as soon as new data is availible for them
 # In case all wend down well, you recieve your output
 # In case some tasks failed, Hamake output error message and tries to finish as many tasks as it can

To get familiar with hamake-file syntax please visit TODO:link_to_hamake_file_syntax_page page. 

==Direct Acyclic Graph==

Frankly speaking, Hamake does not strictly follows [http://en.wikipedia.org/wiki/Dataflow Dataflow] programming paradigm. Hamake runs tasks based on _Direct Acyclic Graph_ (DAG) that it builds at the beginning. Graph is build based on task input and output where graph edge is data and graph node is a task. If output of task A intercepts with input of task B the task B is considered to be dependent on task A. Tasks are then executed in order from root nodes to leaf nodes. Task is considered to be ready for execution as soon as its parents have finished processing data. 

Hamake builds and runs DAG in a following way:
 # At the beginning Hamake loads all tasks from hamake-file into memory and searches for tasks that has no input or input data that does not intercept with other tasks output. This tasks becomes _root tasks_.
 # Hamake selects root tasks to start with and launches them in parallel
 # As soon as some task has finished, it is marked as completed and the child task is launched
 # When no tasks are left, Hamake exits 

Graph is stored in the memory of the machine where Hamake is running, so the number of tasks are limited by the upper bound of the JVM heap.

You can control how many root tasks are selected by two ways:
 * You can list leaf tasks you would like to end with as Hamake command-line parameters
   Example: {{{hadoop jar hamake-j-1.0.jar jar-listings filter-listings}}}
 * You can specify default leaf task you would like to end with in hamake-file. This attribute is taken into account only in case you have not specified leaf tasks to start with as Hamake command-line parameters.
   Example: {{{<project name="test" default="jar-listings">}}}

==Hamake Dependency Mechanism==

In Hamake first task depends on second if input data of first task is an output data of a second. If first task depends on second it will be launched as soon as second task has finished. Tasks may be skipped if they have been executed earlier and thier output is fresh enough. In other words, Hamake runs tasks incrementally, task won't be executed if input data are older than output data.

Lets imagine you have three map-reduce jobs you would like to combine in a workflow. First job collects log files from your servers on HDFS. Second job extracts top N of most active users and puts this list in a distributed cache. Third job counts recommendations for a user based on extracted user list and collected log files. In dataflow programming model you can put third job dependent on the second job by specifying that output folder of the second job will be an input of the third job. But how can you put third job dependent on the first one?

In Hamake you can also define some task to be dependent on external resources. In case of such dependency task will be executed only if external data is present and fresh. Also please note that external resources will not get into task input. Thus, you can define third task to be dependent on the first one by specifying its output folder as external resource.

Each task is executed in a separate thread, therefore tasks which does not depends on each other will be executed in parallel.

In case some task has failed, Hamake will not execute subsequent tasks from the same DAG branch. For example, if graph there are two independent branches in a DAG with the root task A:

http://code.google.com/p/hamake/source/browse/trunk/images/manualFigure1.gif

In case failure has occurred during execution of task B Hamake will not execute task C but  will execute D and E tasks

== Hamake Tasks ==

Hamake operates with three types of tasks which are implemented by user. Threse are Hadoop MR jobs, Pig scripts and arbitrary program or bash script that is launched on a machine where Hamake is running. You define these tasks in hamake-file. 

Example: 
TODO: add example of three types of tasks


==Tasks Submission==

Currently you can define following types of tasks:
|| *Task* || *What Hamake Does* ||
|| Hadoop map-reduce job || Hamake copies MR job jar to local temporary location and then submits job to JobTracker. ||
|| pig || Hamake copies PigLatin script to local temporary location and then executes this script.||
|| exec || Executes specified script or program locally on the machine, where Hamake is running ||

==Working directory==
Hamake resolves all non-absolute paths defined in hamake-file relative to working directory. Working directory can be defined as the Hamake command-line argument with _-w_ option.
   Example: {{{hadoop jar hamake-j-1.0.jar -w /home/hadoop/scripts}}}

If working directory has been omitted, it will be determined by Hamake as:
 * user home directory, if Hadoop default FS is local FS (`file://`)
 * /user/<user name>, if Hadoop default FS is HDFS (hdfs://)

==Paths==
Hamake takes into account schema part of the paths used in workflow script. Thus, for example, if Hamake works on Amazon Elastic MapReduce, it is possible to specify paths to data stored on S3:
{{{
<property name="output"  value="s3://hamake/test/"/>
}}}

=Running Hamake=
If you installed Hamake as described in _Installation_ you can run Hamake from the command-line in a following way:
{{{
hadoop jar path_to_hamake.jar/hamake-j.jar path_to_hamake-file
}}}

To run Hamake you should have Hadoop installed and bin folder of Hadoop should be added to system PATH environment variable.

==Command-line options summary==

hadoop jar path_to_hamake.jar/hamake-j.jar path_to_hamake-file [options] [target [target2] [target3] ...]
Options:
    -V, -version display version and exit
    -v, -verbose verbose mode
    -n, -dry-run execution simulation without actually executing any tasks or modifying any files.
    -t, -test mostly for developers - make it print detailed stack traces in case of errors
    -j, -jobs execute up to N tasks simultaneously. Applies only to independent tasks. By default N is unlimited. -j 1 will make it execute tasks one by one.
    -f, -file specifies makefile name. If this option is omitted 'hamakefile.xml' is assumed
    -w, -workdir path to working directory