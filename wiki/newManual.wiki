=== Hamake Manual ===
version 1.0

== Overview ==

Hamake is a lightweight utility and workflow engine for running Hadoop Map Reduce jobs and Pig scrips on a dataset stored on HDFS. 

== Installation ==

== Running Hamake ==
Hamake runs in Hadoop environment. Please make sure you have installed Hadoop on the machine where you are going to run Hamake and added Hadoop bin directory in PATH environment variable.

{{{
export PATH=$PATH:$HADOOP_HOME/bin
}}}
To run Hamake 
    Example: {{{hadoop jar hamake-j-1.0.jar}}}


Basically, the Hamake utility simply executes series of user-defined code that submits MapReduce jobs to the Hadoop cluster. The Hamake is responsible for analyzing and collecting input data for user-defined MR code. It not allows user to send Map or Reduce jobs separately. 

== Hamake tasks ==
The Hamake operates by the tasks which has to be implemented by the user. These tasks declared in the [newSyntaxReference workflow script] (XML file) and may depend on each other by means of their input and output.
    Example: 
{{{ 
<mapreduceTask name="jar-listings" main="com.codeminders.hamake.examples.JarListing" jar="hamake-examples-1.0.jar">
  <foreach>
    <input name="jar" location="lib" mask="*.jar" />
    <output name="listing" location="jar-listings" />
  </foreach>
  <parameter value="${jar}" />
  <parameter value="${listing}" />
</mapreduceTask>

<mapreduceTask name="filter-listings" main="com.codeminders.hamake.examples.JarListingFilter" jar="hamake-examples-1.0.jar">
  <foreach>
    <input name="jar" location="jar-listings"/>
    <output name="listing" location="jar-listings-filtered" />
  </foreach>
  <parameter value="${jar}" />
  <parameter value="${listing}" />
</mapreduceTask>

}}}

In the example above task "filter-listings" depends on result of "jar-listings" task, so it will be executed just after results of "jar-listings" will be obtained. Easy to see that tasks connected in such a way form [http://en.wikipedia.org/wiki/Digraph_%28mathematics%29 directed graph] structure, where vertexes are defined by the tasks, dependencies define links and their direction. 

[http://upload.wikimedia.org/wikipedia/commons/0/08/Directed_acyclic_graph.png]

Link direction points to the root task (head vertex) that must be executed prior to the current task (tail vertex).

== Task execution ==
In general, the Hamake not strictly follow [http://en.wikipedia.org/wiki/Dataflow Dataflow] programming paradigm. It means the Hamake builds the execution path on base of tasks dependencies graph, so the time of task execution is due to the data state but not the sequence of the tasks declared in the XML script. 

Here how Hamake defines tasks for executing:
 # traverse tasks dependencies graph from start vertexes towards parents, discovering tasks with no input or where input already available
 # execute found tasks and mark they as completed
 # traverse tasks dependencies graph from start vertexes towards parents, discovering completed tasks
 # get incomplete children of found tasks
 # execute child tasks and mark they as completed
 # if graph has incomplete task(s) repeat from step 3

Start vertexes can be defined by two ways:
 # list of start tasks can be specified in Hamake command-line parameters
   Example: {{{hadoop jar hamake-j-1.0.jar jar-listings filter-listings}}}
 # {{{default}}} attribute of {{{project}}} element in the Hamake  [newSyntaxReference workflow script] specifies the only start task. This attribute is taken into account only in case start tasks were note specified in the Hamake command-line.
   Example: {{{<project name="test" default="jar-listings">}}}

Besides task-to-task dependencies, Hamake allows to set up dependency on external resources. Such dependency defines set of file system paths treated as additional task input. Task will be executed only if all its inputs contain fresh data.

A few words about data freshness...<br>
Execution of some tasks may be omitted if they were executed early and inputs were not changed. In other words, Hamake executes tasks incrementally, it means the task won't be executed if input data are older than output data. 

Failed task stops subsequent execution of its branch. For example, if graph is tree of two independent branches, the root in point A, <br>
first branch:  A<-B<-C<br>
second branch:  A<-D<-E<br>
Assume, failure occurred in vertex B, in this case:
 * task at vertex C will not be executed
 * tasks D and E will be executed

Tasks dependencies graph is entirely stored in the memory of the machine where Hamake is executed, so the number of tasks are limited by the resource of this machine as well as by the JVM heap upper limit (2Gb).

Each task is executed in separate thread, therefore tasks with no dependencies can be executed in parallel.

Some tasks can be executed in parallel, if they have fresh input data and not depend to each other.


At the moment following tasks variety available:
|| *Task* || *Description* ||
|| mapreduce || Launches MapReduce job ||
|| pig || Launches PigLatin script ||
|| exec || Executes script or program locally within the running VM (Hamake) ||

*describe how FSs are recognized in XML?*<br>
*external data dependencies*<br>
*task types*<br>
*cycle dependencies*<br>
*working directory*
*if task failed*

== Command Line Parameters ==

=== Using additional libraries and resources ===