=== Hamake Manual ===
version 1.0

== Overview ==

Hamake is a lightweight utility and workflow engine for running Hadoop Map Reduce jobs and Pig scrips on a dataset stored on HDFS. 

== Installation ==

== Running Hamake ==
Hamake runs in Hadoop environment. Please make sure you have installed Hadoop on the machine where you are going to run Hamake and added Hadoop bin directory in PATH environment variable.

{{{
export PATH=$PATH:$HADOOP_HOME/bin
}}}
To run Hamake 
    Example: {{{hadoop jar hamake-j-1.0.jar}}}


Basically, the Hamake utility simply executes series of user-defined code that submits MapReduce jobs to the Hadoop cluster. The Hamake is responsible for analyzing and collecting input data for user-defined MR code. It not allows user to send Map or Reduce jobs separately. 

== Hamake tasks ==
The Hamake operates by the tasks which has to be implemented by the user. These tasks declared in the [newSyntaxReference XML file] and may depend on each other by means of their input and output.
    Example: 
{{{ 
<mapreduceTask name="jar-listings" main="com.codeminders.hamake.examples.JarListing" jar="hamake-examples-1.0.jar">
  <foreach>
    <input name="jar" location="lib" mask="*.jar" />
    <output name="listing" location="jar-listings" />
  </foreach>
  <parameter value="${jar}" />
  <parameter value="${listing}" />
</mapreduceTask>

<mapreduceTask name="filter-listings" main="com.codeminders.hamake.examples.JarListingFilter" jar="hamake-examples-1.0.jar">
  <foreach>
    <input name="jar" location="jar-listings"/>
    <output name="listing" location="jar-listings-filtered" />
  </foreach>
  <parameter value="${jar}" />
  <parameter value="${listing}" />
</mapreduceTask>

}}}

In the example above task "filter-listings" depends on result of "jar-listings" task, so it will be executed just after results of "jar-listings" will be obtained. Easy to see that tasks connected in such a way form [http://en.wikipedia.org/wiki/Digraph_%28mathematics%29 directed graph] structure, where vertices are defined by the tasks and dependencies define links and their direction. 

[http://upload.wikimedia.org/wikipedia/commons/0/08/Directed_acyclic_graph.png]

Link direction points to the root task (head vertice) that must be executed prior to the current task (tail vertice).

== Task execution ==
In general, the Hamake not strictly follow [http://en.wikipedia.org/wiki/Dataflow Dataflow] programming paradigm. It means the Hamake builds the execution path on base of dependencies between output of one tasks and input of others. 

Hence, the time of task execution is due to the data state but not the sequence of the tasks declared in the XML script. Execution of some tasks may be omitted if they are were executed early and input data was not changed. In other words, Hamake executes tasks incrementally, it means the task won't be executed if input data are older than output data. 

Some tasks can be executed in parallel, if they have fresh input data and not depend to each other.

The Hamake builds dependencies tree of tasks outputs and inputs and entirely stored the tree in memory of the machine where Hamake is executed, so the number of tasks are limited by the resource of this machine as well as JVM heap upper limit (2Gb).

At the moment following tasks variety available:
|| *Task* || *Description* ||
|| mapreduce || Launches MapReduce job ||
|| pig || Launches PigLatin script ||
|| exec || Executes script or program locally within the running VM (Hamake) ||

*describe how FSs are recognized in XML?*<br>
*task dependencies*<br>
*external data dependencies*<br>
*task types*<br>
*execution part of the graph*<br>
*default target*<br>
*cycle dependencies*<br>
*working directory*

== Command Line Parameters ==

=== Using additional libraries and resources ===