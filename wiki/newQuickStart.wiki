<wiki:toc max_depth="2" />

=Introduction=

Hamake is a lightweight workflow engine for Hadoop. Hamake helps to organize your Hadoop Map Reduce jobs, Pig script and local programs and launch them based on dataflow principles - your tasks will be executed as soon as new data will be availible for them. Hamake will let you to describe and execute your Hadoop map-reduce jobs, Pig scripts or local scripts along with their input and output data using dataflow approach.

==Features==

   # Lighweight utility - no need of complex installation
   # Based on dataflow programming model
   # Easy learning curve.

==Installation==

To install Hamake simply copy hamake-j-1.0.jar to the directory of your choise and make sure that hadoop command is in your $PATH variable.

{{{
cp hamake-j-1.0.jar /tmp
export PATH=$PATH:$HADOOP_HOME/bin
}}}

=Quick Start=

==Running Hamake Example==
Hamake runs Hadoop MR jobs, Pig scripts and local scripts based on hamake-file.
As an example you can take class-size.xml hamake-file from example folder of the Hamake distribution. To generate data for class-size.xml example, launch following script from the root of Hamake distribution:
{{{
#!/bin/sh
hadoop fs -rmr /user/$USERNAME/build
hadoop fs -rmr /user/$USERNAME/dist
hadoop fs -rmr /user/$USERNAME/lib
hadoop fs -rmr /user/$USERNAME/test

hadoop fs -mkdir dist
hadoop fs -mkdir build
hadoop fs -mkdir lib
hadoop fs -mkdir test/resources/scripts

hadoop fs -put examples/*.jar dist
hadoop fs -put lib/*.jar lib
hadoop fs -put *.jar dist
hadoop fs -put examples/scripts/*.pig test/resources/scripts
}}}
that script will put content of the ''lib'' folder, hamake-examples-1.0.jar and Pig script on HDFS.
Now you can launch hamake:
{{{
hadoop jar your_path_to_hamake/hamake-j-1.0.jar -f path_to_class_size_file/class-size.xml
}}}
In case of successful execution, you should see /user/$USERNAME/build/test/class-size-histogram/part-00000 and /user/$USERNAME/build/test/class-size-median-bin/part-00000 files on HDFS

== Running Hamake Example on Amazon Elastic MR ==

It is possible to run the Hamake on [http://aws.amazon.com/elasticmapreduce/ Amazon EMR]. This use case assumes that all data (input and output), MR code and Hamake workflow script are stored on Amazon S3.
In order to run Hamake example on Amazon EMR, follow the steps below:
  # set up an environment on S3
{{{
copy <distributive>/lib/*.jar to s3n://hamake/lib
copy <distributive>/hamake-j-1.0.jar to s3n://hamake/
copy <distributive>/examples/hamake-examples-1.0.jar to s3n://hamake/
copy <distributive>/examples/hamakefiles/class-size-s3.xml to s3n://hamake/
}}}
  # [http://developer.amazonwebservices.com/connect/entry.jspa?externalID=2264&categoryID=266 download] and [http://docs.amazonwebservices.com/ElasticMapReduce/latest/DeveloperGuide/index.html?introduction.html install] Amazon EMR command-line tool
  # start the Hamake on Amazon EMR:
{{{
elastic-mapreduce --create --jar s3n://hamake/hamake-j-1.0.jar --main-class com.codeminders.hamake.Main --args -t,-v,-f,s3n://hamake/class-size-s3.xml
}}}

Hamake is executed as single job within Amazon EMR job flow, it uses Amazon's Hadoop configuration and submits MR jobs directly to !JobTracker. At the moment Hamake allows to execute only MR tasks within Amazon EMR.

==Creating Your First Hamake-File==
This example will give you a short introduction to hamake-file syntax. To get more insight into it, please visit TODO:link_to_hamakefile-syntax-reference.

First of all, if you haven't done it already, please install hamake. After that create file _first_hamakefile.xml_ with following content:
{{{
<project name="hamakefile-startup">
	<property name="tempDir" value="/tmp" />
	<fold name="fold1">
		<description>list all files in a directory</description>
		<input>
			<file path="${tempDir}/hamake-tmp" />
		</input>
		<output>
			<file path="${tempDir}/out.txt" />
		</output>
		<exec binary="ls">
			<parameter value="${input}"/>
			<constparam value=">" />
			<parameter value="${output}"/>
		</exec>
	</fold>	
	<foreach name="foreach1">
		<description>copy files from one folder to another</description>
		<dependencies />
		<input>
			<fileset path="${tempDir}/in" />
		</input>
		<output>
			<identity path="${tempDir}/hamake-tmp" />
		</output>
		<exec binary="cp">
			<parameter value="${input}"/>
			<parameter value="${output}"/>
		</exec>
	</foreach>
</project>
}}}

This hamake-file starts with `<project>` tag. Whithin _project_ element you can change base directory from _/tmp_ to the directory of your choise in _tempDir_ property. 

This simple hamake-file has only 2 transformation rules:
 * _foreach_ that copies all files from /tmp/in folder to /tmp/hamake-tmp folder
 * _fold_ that lists all file in /tmp/hamake-tmp folder and outputs result of _ls_ command execution to /tmp/out.txt file. 

Each transformation rule is described in `<descrition>` tag and has defined input and output data whithin `<input>` and `<output>` tags. For every transformation rule you define task that will be launched as `<exec>` task 



To run the example execute command:
hadoop jar ```pwd```/hamake-j-1.0.jar -f ```pwd```/first_hamakefile.xml

you should see following output:

TODO: add output as soon as new syntax will be implemented

Please note that _foreach1_ transformation rule has been launched before _fold1_. This has happened, because _foreach1_ depends on _fold1_