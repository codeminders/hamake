===Introduction===

Hamake is a lightweight utility for running Hadoop Map Reduce jobs and Pig scrips on a dataset stored on HDFS. Dataset could be either individual files or directories containing groups of files. Hamake helps to organize your Hadoop Map Reduce jobs and Pig script and launch them based on dataflow programming model - that means that your tasks will be executed as soon as new data will be availible for them. To start working with Hamake all you need is to describe your tasks along with their inputs and outputs locations in ''hamakefile'' using simple XML syntax.

===Features===

   1. lighweight utility - no need of complex installation
   2. based on dataflow programming model
   3. syntax and behavour is similar to Apache Ant

===Installation===

To install Hamake simply copy hamake-j-1.0.jar to the directory of your choise and make sure that hadoop command is in your $PATH variable.

===Quick Start===

First you should create your hamakefile that desribes tasks, tasks input and output data. Syntax of hamakefile is described here: LINK_TO_THE_HAMAKEFILE_SYNTAX.
As an example you can take class-size.xml file from example folder of the Hamake distribution. To generate data for class-size.xml example, launch following script from the root of Hamake distribution:
{{{
#!/bin/sh
hadoop fs -rmr /user/$USERNAME/build
hadoop fs -rmr /user/$USERNAME/dist
hadoop fs -rmr /user/$USERNAME/lib
hadoop fs -rmr /user/$USERNAME/test

hadoop fs -mkdir dist
hadoop fs -mkdir build
hadoop fs -mkdir lib
hadoop fs -mkdir test/resources/scripts

hadoop fs -put examples/*.jar dist
hadoop fs -put lib/*.jar lib
hadoop fs -put *.jar dist
hadoop fs -put examples/scripts/*.pig test/resources/scripts
}}}
that script will put content of the ''lib'' folder, hamake-examples-1.0.jar and Pig script on HDFS.
Now you can launch hamake:
{{{
hadoop jar your_path_to_hamake/hamake-j-1.0.jar -f path_to_class_size_file/class-size.xml
}}}
In case of successful execution, you should see /user/$USERNAME/build/test/class-size-histogram/part-00000 and /user/$USERNAME/build/test/class-size-median-bin/part-00000 files on HDFS

== Running on Amazon Elastic MR ==

It is possible to run the Hamake on [http://aws.amazon.com/elasticmapreduce/ Amazon EMR]. This use case assumes that all data (input and output), MR code and Hamake workflow script are stored on Amazon S3.
In order to run Hamake example on Amazon EMR, follow the steps below:
  # set up an environment on S3
{{{
copy <distributive>/lib/*.jar to s3n://hamake/lib
copy <distributive>/hamake-j-1.0.jar to s3n://hamake/
copy <distributive>/examples/hamake-examples-1.0.jar to s3n://hamake/
copy <distributive>/examples/hamakefiles/class-size-s3.xml to s3n://hamake/
}}}
  # [http://developer.amazonwebservices.com/connect/entry.jspa?externalID=2264&categoryID=266 download] and [http://docs.amazonwebservices.com/ElasticMapReduce/latest/DeveloperGuide/index.html?introduction.html install] Amazon EMR command-line tool
  # start the Hamake on Amazon EMR:
{{{
elastic-mapreduce --create --jar s3n://hamake/hamake-j-1.0.jar --main-class com.codeminders.hamake.Main --args -t,-v,-f,s3n://hamake/class-size-s3.xml
}}}

Hamake is executed as single job within Amazon EMR job flow, it uses Amazon's Hadoop configuration and submits MR jobs directly to JobTracker. At the moment Hamake allows to execute MR tasks only within Amazon EMR.