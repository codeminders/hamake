\documentclass{article}
\usepackage{verbatim}
\usepackage{moreverb}
\usepackage{url}
\usepackage{amsmath}
\usepackage{color}
\usepackage{appendix}
\author{Vadim Zaliva lord@crocodile.org, Vladimir Orlov vorl@codeminders.com}
\date{2010}
\title{Dataflow Approach to Data Processing in Hadoop with HAMAKE}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{rotating}
\usepackage[colorlinks=true,bookmarks=true,pdfauthor={Vadim Zaliva lord@crocodile.org Vladimir Orlov vorl@codeminders.com},
            pdftitle={Dataflow Approach to Data Processing in Hadoop with HAMAK},
            pdftex]{hyperref}

\input{graphviz}
\begin{document}
\lstset{language=XML,basicstyle=\tiny,markfirstintag=true,numbers=left, numberstyle=\tiny}

\maketitle

\begin{abstract}
  Most non-trivial data processing scenarios with Hadoop typically
  require more than one MapReduce job. Usually such processing is
  data-driven, with the data funneled through a sequence of jobs. The
  processing model could be presented in terms of dataflow
  programming. It could be expressed as a directed graph, with
  datasets as nodes. Each edge indicates a dependency between two or
  more datasets and is associated with a processing instruction
  (Hadoop MapReduce job, PIG Latin script or an external command),
  which produces one dataset from the others. Using fuzzy timestamps
  as a way to detect when a dataset needs to be updated, we can
  calculate a sequence in which the jobs need to be executed to bring
  all datasets up to date. Jobs for updating independent datasets
  could be executed concurrently, taking advantage of your Hadoop
  cluster's full capacity. The dependency graph may even contain
  cycles, leading to dependency loops which could be resolved using
  dataset versioning.

  These ideas inspired the creation of \textbf{hamake} utility. We tried
  emphasizing data and allowing the developer to express one's goals
  in terms of dataflow (versus workflow). Data dependency graph is
  expressed using just two data flow instructions: fold and foreach
  providing a clear processing model, similar to MapReduce, but on a
  dataset level. Another design goal was to create a simple to use
  utility that developers can start using right away without complex
  installation or extensive learning. We think we have been able to
  achieve these goals and the utility we have created has been
  successfully used in real-life large scale Hadoop data processing
  projects. 
\end{abstract}

\section{Introduction}

MapReduce model introduced by Google\cite{dean2008map} allows
processing of large data amounts. Hadoop\cite{bialecki2005hadoop} is
popular open-source implementation of MapReduce.

Hadoop is usually used to proccess big amounts of data. The
consequence of this is that even trivial tasks are generally I/O-bound
\cite{hadoopattwitter},\cite{hs2010hadoopbench} which put special
emphasis on incremental processing.

To address the problem of incremental processing of big data sets in
collaborative filtering system Python-language prototype of
\textbf{hamake} was developed in late 2008. Based on our experience
with the prototype, we built a new version of the product which was
used in several subsequent projects for such tasks as processing of
web crawling results (text analysis) and Hadoop peformance
testing. Re-written from scratch in Java, the new version has many
improvements like streamlined syntax of a dataflow definition file,
Amazon EMR support and others.

\textbf{hamake} is open source, disributed under Apache License
v2.0. The project is hosted at google code at the following URL:
\url{http://code.google.com/p/hamake/}.

\section{Processing Model}

\textbf{hamake} operates on \textit{filesets} which consists of indvidual
\textit{files}. Fileset defines group of files, residing on filesystem
accessible from Hadoop task (usually DFS). \textit{Dataflow
  transformation rules (DTR)} specify operations which could take some
files or filesets as inputs (or dependecies) and possibly produce
other filesets as output.

If file \textit{A} is listed as input of a DTR, and file \textit{B}
listed as output of the same DTR, it is said that \textit{``B depends
  on A''}. \textbf{hamake} is using file time stamps for dependency
calculation. A time stamp of a file is date and time when it was last
modified.  In hamke world, file system directory or folder is also a
file, and has its own time stamp. DTR outputs said to be \textit{up to
  date} if minimal time stamp on all outputs is greater or equal than
maximum timestamp on all inputs (and dependency inputs). Groups of
files and folders could be also organized into the \emph{sets} which
could be also used as dependencies. See hamake syntax
refererence\cite{hamakesyntax} for full syntax.

A notion of \textit{fuzzy timestamp}\footnote{\textit{fuzzy timestamp}
  support is not included in current version of \textbf{hamake}.}
allows us to compare two time stamps allowing certain margin or
error. The ``fuzziness'' is controlled by tolerance
$\sigma$. Timestamp $a$ is considered to be older than timestamp $b$
if $b-a>\sigma$. Setting $\sigma=0$ gives us non-fuzzy, strict
timestamp comparison.

\textbf{hamake} attempts to ensure that all DTR outputs are up to
date\footnote{we do not try to ensure that files which are not listed
  as DTR outputs are up to date, becase we have no way to update
  them}. To do so it builds a \textit{dependency graph}. This graph
have files or filesets as vertices and DTRs as edges. We will see
below that this is guaranteed to be a \textit{direct acyclic graph}
(DAG).

Then, the graph reduction algorithms shown on Figure~\ref{fig:grred}
is performed.

\begin{figure}[htp]
\centering
\includegraphics[width=10cm]{GraphReduction.png}
\caption{\textbf{hamake} dependency graph reduction algorithm}
\label{fig:grred}
\end{figure}

Essentially, this a version of Kahn's
algorithm\cite{kahn1962topological}, implementing topological
ordering\cite{wiki:topsort}.

Some steps could be executed in parallel: if more than one DTR without
input dependencies is found on step 1, subsequent steps 2-5 could be
executed for each discovered DTR in parallel.

It should be noted that if DTR exectuion has failed, we still can and
will process other DTRs which do not depend directly or indirectly on
results of this DTR. This allow us to fix the problem and re-run
\textbf{hamake}, without need to re-process all data.

Cyclic dependences has to be avoided, because workflow contaning them
is not guaranteed to terminate. Implicit checks are perfomed during
reading hamakefile and building dependency graph. If a cycle is
detected, it is reported as an error. So the dependency graph used by
\textbf{hamake} is \textit{directed acyclic graph}.

However \textbf{hamake} supports a limited scenario of interative processing,
where some output of one iteration are used as an input in the next
one. We do not support more general case of \textit{fixed point}
iterative calculations: each run of \textbf{hamake} is a signle pass. This
mechanism is implemented via \textbf{hamake} feature called
\textit{generations}. Each input or output file could be optionally
marked with \emph{generation} attribute. Same file with with different
generation numbers is treated as different vertex is dependency
graph. (TODO: elaborate)

\section{Scheduler}

The design goal of \textbf{hamake} scheduler to perform all required
calculations in shortest possible time. Obviously to achive this it
should aim for maximal cluster utilization, performing as many
calculations in parallel as possible.

There are three factors driving scheduling loginc: DTR dependencies,
files, and Hadoop tasks.

On highest level DTR dependencies determine sequence of DTRs jobs
being launched. In case of \emph{fold} DTR, normally a single Hadoop
job, PIG script or shell command is launched. However in case of
\emph{foreach} DTRs, an additional level of parallelism could be
achived, as individual jobs will be launched for each input file.

\begin{figure}[htp]
\centering
\includegraphics[width=6cm]{twofold.png}
\caption{Simple \emph{fold} DTR}
\label{fig:fold1}
\end{figure}

In example shown on Figure~\ref{fig:fold1}, since fileset \textit{B}
depends on all files in fileset \textit{A}, there are not many
opportunities to for parallel execution and \emph{fold} DTR is
executued as as single job. At first glance, two dependent
\emph{foreach} shown in Figure~\ref{fig:foreach1} DTRs looks similar.

\begin{figure}[htp]
\centering
\includegraphics[width=7cm]{twoforeach.png}
\caption{Simple \emph{foreach} DTR}
\label{fig:foreach1}
\end{figure}

However \emph{foreach} DTRs works by mapping individual files in
fileset \textit{A} to files in fileset \textit{B}. Assuming that
fileset \textit{A} consists of 3 files: \textit{a1}, \textit{a2},
\textit{a3} the the dependency graph could be represented as shown on
Figure~\ref{fig:foreach2}.

\begin{figure}[htp]
\centering
\includegraphics[width=7cm]{twoforeachp.png}
\caption{Decomposition of \emph{foreach} DTR}
\label{fig:foreach2}
\end{figure}

In this case we have an opportunity to execute three jobs in parallel.

Unltimately, the Hadoop cluster capacity is defined in terms of number
of \textit{map slots} and \textit{reduce slots}.  DTR lauches Hadoop
job (either directly as defined by \emph{mapreduce} instruction or via
PIG script, single job will spawn one or more \emph{mapper} and
\emph{reducer} tasks, each taking one respecive slot. Number of
mappers and reducers lauched depends on many factors: size of DFS
block, Hadoop general settings, \emph{JobConf} settings for particular
job are among them. In general, \textbf{hamake} does not have an neither
visibility nor control over most of these factors, so the \textbf{hamake}
scheduler currently does not deal with invidual tasks. It knows only
about data files and spawned Hadoop jobs.

\section{Example}

Imagine an online library service, that allows to add, search and preview millions
of books from libraries and users worldwide. Looking at the size of a library, it's hardly 
surprising that there's a lot of similar books circulating as well. Until recently, to find
out similar book we limited ourselfs to the tedious business of reading the back 
of every book on the shelves. Now, almost all online libraries can automatically detect similar books. 

Design and implementation of an engine that will be able to detect similar books is not an 
easy task for even a small number of data, but how would you find them in over million of books?
One way to measure similarity between two books is by measuring the cosine of the angle between their feature vectors\cite{wiki:cosinesimilarity}.
The result of the cosine function is equal to 1 when the angle is 0, and it is less than 1 when the angle is 
of any other value. On Figure~\ref{fig:SimilarityAlg1} and Figure~\ref{fig:SimilarityAlg2} you can see high-level scheme of
the algorithm, that can be used to mark similar books from bookstore.

To be able to process thouthands of books, implementation of this algorithm should be scalable and fast.
Today, to reach true scalability and save your time you would probably use Hadoop. For this particular task,
 I would implement several Map Reduce jobs: that would extracts metadata and structured text content from
 various documents using existing parser libraries, create sparce vectors from keywords frequency and select similar books by comparing cosine distance between feature vectors of all books. 
As a result, I will end up with 6 Map Reduce jobs, that could be combined in a direct acyclic graph, the same as on Figure~\ref{fig:SimilarityAlgDAG}

Despite the fact that Hadoop simplifies a lot writing of scalable programs, you still should design and implement a code, that
will trigger execution of a flow of your direct acyclic graph, consisting of Map Reduce jobs on incoming of new books. Also, you should implement a logic,
that will detect incorrect data, and exclude it on next run of the process. This is where Hamake can be used. 

Hamake helps you to organize your Hadoop map-reduce jobs or Pig scripts in a workflow, that will be launched and controlled from a single machine
All you should do is to write an XML descriptor of your flow, and launch Hamake, with descriptor file, passed to it as a first argument. The order
in which your Map-Reduce jobs will run, will be controlled by data, not by tasks themselfs.

To give you an example of how an XML descriptor will look like, lets look at a descriptor of process of finding similar books in Listing~\ref{hamakeFile}
First, for convinience, I've defined properties, that will be used by all data transformation rules (lines 5 - 6). In the same way, I've defined
an input directory with books, an output file where list of similar books will be produced and a file with stop-words (lines 8 - 11).
Next, I've added a definition of the first Map Reduce job that converts a book from native format to plain text. In Hamake, Hadoop jobs, Pig scripts or external process should be
defined as data transformation rule. A data transformation rule can be of two tipes: \textit{foreach} or \textit{fold}. For each data transformation rule you should 
specify its input and output data. My first job (lines 13-28) takes two parameters - an input a path to a book, and a path to a file, where converted book should be written. 
As you can see from input data location is specified within \textit{input} tag and output data location within \textit{output} tag. With help of attributes \textit{jar} and \textit{main} of tag 
\textit{mapreduce} I specified location of a jar file with Hadoop job and main class name. Then, in the same way I've added all five Hadoop Map-Reduce jobs that are left. As you can see, these jobs takes as an input files, produced by the previous task. When you launch Hamake, with XML descriptor from Listing~\ref{hamakeFile}, it will first loads all tasks into memory and searche for tasks that has no input or input data that does not intercept with other tasks output. This tasks (\textit{extractPlainText}), becomes root task. As soon as Hamake has finished execution of the first task,the child job will be launched, and so on, while no jobs are left.
As a result, you will get a file \textit{results.txt} with list of similar books.

\begin{lstlisting}[caption={Hamake file, that describes process for detecting similar books},label=hamakeFile]
<?xml version="1.0" encoding="UTF-8"?>

<project name="cluster books">

  <property name="lib" value="./lib/" />
  <property name="cosine.distance" value="0.02" />

  <fileset id="input" path="/data/" mask="*.pdf" />
  <file id="output" path="./result.txt" />
  <file id="stopwords" path="./stopwords.txt" />
  <file id="temporaryFolder" path="/tmp/" />

  <foreach name="extractPlainText">
    <input>
      <include idref="input" />
    </input>
    <output>
      <file id="bookInPlainTextFormat" path="${temporaryFolder}/plainText/${foreach:filename}" />
    </output>
    <mapreduce jar="${lib}/hadoopJobs.job" main="com.fb2pdf.hadoop.cluster.ConvertFb2Text">
      <parameter>
        <literal value="${foreach:path}" />
      </parameter>
      <parameter>
        <reference idref="bookInPlainTextFormat" />
      </parameter>
    </mapreduce>
  </foreach>

  <foreach name="KeywordsExtractor">
    <input>
      <file id="bookInPlainTextFormat" path="${temporaryFolder}/plainText" />
    </input>
    <output>
      <file id="bookKeywords" path="${temporaryFolder}/keywords/${foreach:filename}" />
      <file id="vacabulary" path="${temporaryFolder}/vacabulary" />
    </output>
    <mapreduce jar="${lib}/fb2pdf.job" main="com.fb2pdf.hadoop.cluster.KeywordsExtractor">
      <parameter>
        <literal value="${foreach:path}" />
      </parameter>
      <parameter>
        <reference idref="bookKeywords" />
      </parameter>
      <parameter>
        <reference idref="vacabulary" />
      </parameter>
    </mapreduce>
  </foreach>

  <foreach name="ExcludeStopword">
    <dependencies>
      <include idref="KeywordsExtractor" />
    </dependencies>
    <input>
      <fileset id="vacabulary" path="${temporaryFolder}/vacabulary" />
    </input>
    <output>
      <file id="vacabularyWithoutStopwords" path="${temporaryFolder}/vacabularyWOStopWords/${foreach:filename}" />
    </output>
    <mapreduce jar="${dist}/fb2pdf.job" main="com.fb2pdf.hadoop.cluster.StopwordsExcluder">
      <parameter>
        <literal value="${foreach:path}" />
      </parameter>
      <parameter>
        <reference idref="vacabularyWithoutStopwords" />
      </parameter>
      <parameter>
        <reference idref="stopwords" />
      </parameter>
    </mapreduce>
  </foreach>

  <fold name="FeatureVectorsFromDirectory">
    <input>
      <file id="vacabularyWithoutStopwords" path="${temporaryFolder}/vacabularyWOStopWords" />
      <file id="keywords" path="${temporaryFolder}/keywords" />
    </input>
    <output>
      <file id="featureVectorsFile" path="${temporaryFolder}/featureVectors" />
    </output>
    <mapreduce jar="${dist}/fb2pdf.job" main="com.fb2pdf.hadoop.cluster.FeatureVectorsFromDirectory">
      <parameter>
        <reference idref="vacabularyWithoutStopwords" />
      </parameter>
      <parameter>
        <reference idref="keywords" />
      </parameter>
      <parameter>
        <reference idref="featureVectorsFile" />
      </parameter>
    </mapreduce>
  </fold>

  <foreach name="OutputResults">
    <input>
      <file id="featureVectors" path="${temporaryFolder}/featureVectors" />
    </input>
    <output>
      <include idref="output" />
    </output>
    <mapreduce jar="${lib}/fb2pdf.job" main="com.fb2pdf.hadoop.cluster.OutputClusterResults">
      <classpath>
        <fileset path="lib" mask="*.jar" />
      </classpath>
      <parameter>
        <literal value="${cosine.distance}" />
      </parameter>
      <parameter>
        <reference idref="featureVectors" />
      </parameter>
      <parameter>
        <reference idref="vectors" />
      </parameter>
      <parameter>
        <reference idref="output" />
      </parameter>
    </mapreduce>
  </foreach>
</project>
\end{lstlisting}

\begin{figure}[htp]
\centering
\includegraphics[width=10cm]{SimilarityAlgDAG.png}
\caption{Directed Acyclic Graph of a Hamake Process for detecting similar books}
\label{fig:SimilarityAlgDAG}
\end{figure}

\section{Related Work}

Table~\ref{table:1} below attempts to compare \textbf{hamake} and similar
workflow engines for Hadoop
(\href{http://github.com/tucu00/oozie1}{Oozie},
\href{http://sna-projects.com/azkaban/}{Azkaban},
\href{http://www.cascading.org/}{Cascading}) based on some key
features. Although all of these systems could be used to solve similar
problems, they differ significantly in design, philosophy, target user
profile, usage scenarios, etc.  So our feature-wise comparison is in
no way conclusive. Please use it as a guideline, but read respective
systems documentation to understand better which one is more suitable
for your problem.

\begin{sidewaystable}
  \begin{tabular}{ | p{3cm} || l | l | l | l |}
    \hline
    \textbf{Feature} & \textbf{hamake} & \textbf{Oozie} & \textbf{Azkaban} & \textbf{Cascading} \\
    \hline
    Workflow discription language & XML & XML (xPDL based) & text file with key/value pairs & Java API \\
    Dependencies mechanism & data-driven & explicit & explicit & explicit \\
    Requires Servlet/JSP container & No & Yes & Yes & No \\
    Allows to track a workflow progress & console/log messages & web page & web page & Java API \\
    Ability to schedule a Hadoop job execution at given time & no & yes & yes & yes \\
    Execution model & command line utility & daemon & daemon & API \\
    Allows to run Pig Latin scripts & yes & yes & yes & yes \\
    Event notification & no & no & no & yes \\
    Requires installation & no & yes & yes & no \\
    Supported Hadoop version & 0.18+ & 0.20+ & currently unknown & 0.18+ \\
    Retries & no & at workflow node level & yes & yes \\
    Ability to run arbitrary commands & yes & yes & yes & yes \\
    Can be run on  Amazon EMR & yes & no & currently unknown & yes \\
    \hline
  \end{tabular}
  \caption{Feature comparison of popular Hadoop workflow engines }
  \label{table:1}
\end{sidewaystable}

Below are some notes, discussing some key differences between \textbf{hamake}
and some other popular products:

\subsection*{Cascading}

In short: Cascading is an API, while \textbf{hamake} is an utility. Some differences:
\begin{itemize}
\item \textbf{hamake} does not require any custom programming. It helps to automate running your existing Hadoop tasks and PIG scripts
\item You can use \textbf{hamake} to automate tasks written in other languages, for example using \textit{Hadoop streaming}
\end{itemize}

\subsection*{Oozie and Azkaban}

Oozie and Azkaban are server-side systems that have to be installed
and run as a service. \textbf{hamake} is a lightweight client-side utility that
does not require installation and has very simple syntax for workflow
definition.  Most importantly, \textbf{hamake} is built based on dataflow
programming principles - your Hadoop tasks execution sequence is
controlled by the data.
 

\section{Future Work}

Better integration with Hadoop schedulers (Fair Scheduler, Capacity
Scheduler, etc.).

\nocite{*}
\bibliography{hamake}
\bibliographystyle{unsrt}

\end{document}

