\documentclass[10pt,conference,letterpaper]{IEEEtran}
\usepackage{verbatim}
\usepackage{moreverb}
\usepackage{url}
\usepackage{amsmath}
\usepackage{color}

\title{Dataflow Approach to Data Processing in Hadoop with HAMAKE}

\author{\IEEEauthorblockN{Vadim Zaliva}
\IEEEauthorblockA{Codeminders\\
Email: lord@crocodile.org} \and \IEEEauthorblockN{Vladimir Orlov}
\IEEEauthorblockA{Codeminders\\
Email: vorl@codeminders.com}}

\date{2011}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{rotating}
\usepackage[colorlinks=true,bookmarks=true,pdfauthor={Vadim Zaliva lord@crocodile.org Vladimir Orlov vorl@codeminders.com},
            pdftitle={Dataflow Approach to Data Processing in Hadoop with HAMAK},
            pdftex]{hyperref}

\input{graphviz}
\begin{document}
\lstset{language=XML,basicstyle=\tiny,markfirstintag=true}

\maketitle

\begin{abstract}
  Most non-trivial data processing scenarios using Hadoop typically
  involve launching more than one MapReduce job. Usually, such
  processing is data-driven, with the data funneled through a sequence
  of jobs. The processing model could be expressed in terms of
  dataflow programming.
  
  It could be represented as a directed graph, with datasets as
  vertices. Using \textit{fuzzy timestamps} as a way to detect which
  dataset needs to be updated, we can calculate a sequence in which
  Hadoop jobs need to be launched to bring all datasets up to
  date. Incremental data processing and parallel job execution fit
  well into this approach.

  These ideas inspired the creation of \textbf{hamake} utility. We
  attempted to emphasize data and allowing the developer to formulate
  his problem as a data flow (opposed to a workflow approach commonly
  used). Hamake language using just two data flow operators:
  \emph{fold} and \emph{foreach} providing a clear processing model,
  similar to MapReduce, but on a dataset level.
\end{abstract}

\section{Motivation and History}

MapReduce data processing model have been introduced by
Google\cite{dean2008map}. Hadoop\cite{bialecki2005hadoop} is popular
open-source implementation of MapReduce.

Hadoop is typically used to process big amounts of data via series of
relatively simple operations. Usually Hadoop jobs are I/O-bound
\cite{hadoopattwitter},\cite{hs2010hadoopbench}, and execution of even
trivial operation on large dataset could take significant system
resources. This makes incremental processing important. Or initial
inspiration was Unix \emph{make} utility. While applying some of ideas
implemented in it to Hadoop, we took an opportunity to generalise
processing model in terms of dataflow programming.

\textbf{hamake} was developed in late 2008 to address the problem of
incremental processing of big data sets in a collaborative filtering
project.

We've striven to create a simple to use utility that developers can
start using right away without complex installation or extensive
learning. 

\textbf{hamake} is open source, distributed under Apache
License v2.0. The project is hosted at google code at the following
URL: \url{http://code.google.com/p/hamake/}.

\section{Processing Model}

\textbf{hamake} operates on \textit{files}, residing on local or
distributed file system accessible from Hadoop job (usually it is
HDFS). Each file has a timestamp, reflecting date and time of its last
modification. A file system directory or folder is also a file, and
has its own timestamp. A \textit{Data Transformation Rule (DTR)}
defines an operation which takes files or filesets as inputs and
produce other files or filesets as outputs.

If file \textit{A} is listed as input of a DTR, and file \textit{B}
listed as output of the same DTR, it is said that \textit{``B depends
  on A''}. \textbf{hamake} is using file time stamps for dependency
up-to-date checks. DTR outputs said to be \textit{up to date} if
minimum time stamp on all its outputs is greater or equal than maximum
timestamp on all inputs. A user, for his or her convenience, could
arrange groups of files and folders into a \emph{fileset} which later
could be references as DTR's input or output.

Hamake is using \textit{fuzzy timestamps}\footnote{current stable
  version of \textbf{hamake} is using exact (non-fuzzy) timestamps.}
which could be compared, allowing for a certain margin or error. The
``fuzziness'' is controlled by tolerance $\sigma$. Timestamp $a$ is
considered to be older than timestamp $b$ if $b-a>\sigma$. Setting
$\sigma=0$ gives us non-fuzzy, strict timestamp comparison.

\textbf{hamake} attempts to ensure that all DTR outputs are up to
date\footnote{It does not try to ensure that files which are not
  listed as one of DTR outputs are up to date, because it has no way
  to update them.}.  To do so it builds a \textit{dependency
  graph}. This graph have individual files or filesets as vertices and
DTRs as edges. We will see below that this is guaranteed to be a
\textit{Direct Acyclic Graph} (DAG).

Then, a graph reduction algorithms shown on Figure~\ref{fig:grred} is
executed. It is similar to Kahn's algorithm\cite{kahn1962topological},
of topological ordering.

\begin{figure}[htp]
\centering
\includegraphics[width=4cm]{GraphReduction.eps}
\caption{\textbf{hamake} dependency graph reduction algorithm}
\label{fig:grred}
\end{figure}

The algorithms allows for parallelism. If more than one DTR without
input dependencies is found during step 1, subsequent steps 2-5 could
be executed for each discovered DTR in parallel.

It should be noted that if DTR exectuion has failed, hamake still can
and will process other DTRs which do not depend directly or indirectly
on results of this DTR. This permits user to fix the problem later and
re-run \textbf{hamake}, without need to re-process all data.

Cyclic dependencies has to be avoided, because a dataflow containing
such dependencies is not guaranteed to terminate. Implicit checks are
performed during reading DAG definitions and building dependency
graph. If a cycle is detected, it is reported as an error. So the
dependency graph used by \textbf{hamake} ensured to be a
\textit{directed acyclic graph}.

However \textbf{hamake} supports a limited scenario of iterative
processing, where DTR output of one hamake execution could used as
another DTR's input during the next hamake run. This mechanism is
implemented via \textbf{hamake} feature called
\textit{generations}. Each input or output file could be optionally
marked with \emph{generation} attribute. The two files with different
generation numbers although referencing the same path on file system
are treated as different vertex is dependency graph. [TODO: example]

One useful consequence of hamake dataflow being a DAG, is that for
each vertex we can calculate list of vertices it depends on, directly
and indirectly using simple \textit{transitive closure}. On datasets,
where the cost of re-calculation could be potentially high (due to
data size or computational complexity) this allows to estimate the
scope of dataflow graph affected by updating one or more ones.

hamake is driven by dataflow description, expressed in simple
XML-based language. The full syntax is described in
\cite{hamakesyntax}. Two main elements \emph{fold} and \emph{foreach},
corresponding to two types of DTRs. Each of them have inputs, outputs
and processing instruction. Executing processing instruction should
bring DTR outputs up to date, compared to its inputs. \emph{fold}
implies many-to-one dependency between inputs and the outputs. In
other words, outputs depends on all inputs and if either of inputs
have been changed, outputs needs to be updated. \emph{foreach} implies
one-to-one dependency: where for each file in input set there is a
corresponding output file, each of them updated independently.

hamake dataflow language has a declarative semantics. Keeping it
purely declarative, makes it easy in future to implement various
dataflow analysis and optimization algorithms on top of it. Some
examples of such algorithms could be: merging dataflows, further
execution parallelization, dataflow complexity analysis and
estimation.

\section{Scheduler}

\textbf{hamake} scheduler is responsible determining the sequence and
launching Hadoop jobs required to bring all datasets up-to-date.  It
attempts to perform all required computations in shortest possible
time. To achieve this it aims for maximal cluster utilization, running
as many Hadoop jobs in parallel as cluster capacity permits.

There are three main factors driving scheduling logic: files,
dependencies, and cluster computational capacity. On highest level DTR
dependencies determine sequence of DTRs jobs being launched.

In case of \emph{fold} DTR, a single Hadoop job, PIG script or shell
could be launched and hence there are not many opportunities to for
parallel execution. In example shown on Figure~\ref{fig:fold1}, since
fileset \textit{B} depends on all files in fileset \textit{A}, a
single job associated with \emph{fold} DTR will be executed.

\begin{figure}[htp]
\centering
\includegraphics[width=0.45\textwidth]{twofold.png}
\caption{Simple \emph{fold} DTR}
\label{fig:fold1}
\end{figure}

At first glance, \emph{foreach} DTR shown in Figure~\ref{fig:foreach1}
looks similar to \emph{fold}.

\begin{figure}[htp]
\centering
\includegraphics[width=0.45\textwidth]{twoforeach.png}
\caption{Simple \emph{foreach} DTR}
\label{fig:foreach1}
\end{figure}

However \emph{foreach} DTRs works by mapping individual files in
fileset \textit{A} to files in fileset \textit{B}. Assuming that
fileset \textit{A} consists of 3 files: \textit{$a_1$},
\textit{$a_2$}, \textit{$a_3$} the the dependency graph could be
represented as shown on Figure~\ref{fig:foreach2}. In this case we
have an opportunity to execute three jobs in parallel.

\begin{figure}[htp]
\centering
\includegraphics[width=0.45\textwidth]{twoforeachp.png}
\caption{Decomposition of \emph{foreach} DTR}
\label{fig:foreach2}
\end{figure}

The Hadoop cluster capacity is defined in terms of number of
\textit{map slots} and \textit{reduce slots}. DTR launches Hadoop job
(either directly as defined by \emph{mapreduce} processing instruction
or via PIG script, single job will spawn one or more \emph{mapper} and
\emph{reducer} tasks, each taking one respective slot. Number of
mappers and reducers lauched depends on many factors, such as size of
HDFS block, Hadoop cluster settings, \emph{JobConf} settings. In
general, \textbf{hamake} does not have an neither visibility nor
control over most of these factors, so the \textbf{hamake} scheduler
currently does not deal with individual tasks. It knows only about
data files and spawned Hadoop jobs.

\section{Example}

Imagine an online library service, that allows to add, search and
preview millions of books from libraries and users worldwide. Looking
at the size of a library, it's hardly surprising that there's a lot of
duplicated titles circulating as well. Such duplicates may be caused
by OCR errors, typos, differences in formatting, or added material such as
foreword or publisher information.

There are known approaches for estimating degree of similarity between
two text documents, but scaling them to work with corpus of millions
of books poses a formidable technical challenge.

This problem is a good match for implementation using Hadoop
framework. It would efficiently distribute computations across cluster
of machines. For the purpose of illustration of hamake usage we will
consider simple, ``brute force'' approach of pairwise comparison of
all texts. This solution does not scale very well, as the
computational complexity is [TODO: complexity using of bigO
notation]. For practical usage more advances clustering algorithms
will be more appropriate.

The implementation could be split into series of steps, each of them
represented by a \textit{MapReduce job}:

\begin{description}[\IEEEsetlabelwidth{\emph{FilterStopwords}}]
\item[\emph{ExtractText}] Extract a plain text from native document format
  (e.g. PDF).
\item[\emph{Tokenize}] Split plain text into a tokens, roughly
  corresponding to words. Dealing with hyphens, compound words,
  accents and diacritics, case-folding.
\item[\emph{FilterStopwords}] Filtering out \textit{stopwords} (e.g. words
  like \textit{a}, \textit{the}, \textit{are}) from the list of
  tokens.
\item[\emph{Normalize}] Stemming or lemmatization of tokens,
  resulting in list of \text{terms}.
\item[\emph{CalculateTF}] Using \textit{vector space
    model}\cite{manning2008introduction} for each text calculate a
  feature vector: a vector of term frequencies.
\item[\emph{FindSimilar}] Find books similar content by comparing
  their feature vectors (for example using \textit{cosine
  distance}).
\end{description}

Each of resulting six MapReduce jobs produces and output file which
depends on input. The have to be invoked sequentially, as outputs of
one tasks are used as inputs of the next one. If one of source files
has been updated, all dependent files have to be re-calculated. These
dependencies could be naturally represented by direct acyclic graph,
show on Figure~\ref{fig:SimilarityAlgDAG}, with vertices representing
data files, and jobs assigned to edges.

Analyzing dependencies and launching required MapReduce jobs could be
automated using hamake. The XML file describing the dataflow is show
as Listing~\ref{hamakeFile}.

First, for convinience, we define properties, that will be used by all
data transformation rules (lines 5 - 6). In the same way, we define an
input directory with books, an output file where list of similar books
will be produced and a file with list of stop-words (lines 8 - 11).
Next, we define the first Map Reduce job that converts a book from
native format to plain text. In Hamake, Hadoop jobs, Pig scripts or
external process should be defined as data transformation rule
(\textit{foreach} or \textit{fold}). For each data transformation rule
you should specify its input and output data. Our first job (lines
13-28) takes two parameters - an input a path to a book, and a path to
a file, where converted book should be written.  As you can see, an
input of the first job is a reference to a file-set of files within
\textit{/data} folder, and output is a set of files with the same
names, but within \textit{/tmp/plainText/} folder. With help of
attributes \textit{jar} and \textit{main} of tag \textit{mapreduce} we
specified a location of a Java archive with Hadoop Map Reduce jobs and
a name of the main class name. Next, in the same way, we add all five
Map-Reduce jobs that are left (lines 30-81). Hamake will determine
jobs order by comparing their input with output of other jobs.

\lstinputlisting[caption={Hamake file, that describes process for
  detecting similar books}, label=hamakeFile]{sample.xml}

Hamake, when launched with this XML descriptor it will first search
for tasks that has no inputs or which inputs are not outputs of other
tasks. In our particular example, this is job with id
\textit{extractPlainText}. Hamake will launch this job first, an as
soon as it will be finished, will execute jobs which depend on outputs
of this task, and so on while all files are up to date. As a result of
this data flow, you will get a file \textit{results.txt} with list of
similar books.

[TODO: an example of partial recalculation]

\begin{figure}[htp]
\centering
\includegraphics[width=0.45\textwidth]{SimilarityAlgDAG.png}
\caption{Directed Acyclic Graph of a Hamake Process for detecting similar books}
\label{fig:SimilarityAlgDAG}
\end{figure}

\section{Related Work}

Several workflow engines exist for Hadoop, such as
\href{http://github.com/tucu00/oozie1}{Oozie},
\href{http://sna-projects.com/azkaban/}{Azkaban}, and
\href{http://www.cascading.org/}{Cascading}.  Although all of these
products could be used to solve similar problems, they differ
significantly in design, philosophy, target user profile, usage
scenarios, etc., limiting usefulness of simple feature-wise
comparison.

The main, most significant difference between these engines and hamake
lies in \textit{workflow} vs. \textit{dataflow} approach. All of them
using the former, explicitly specifying dependencies between
jobs. Hamake, on the other hand, using dependencies between datasets,
to derive the workflow. Both approaches are have their advantages. One
can argue that for some problems a dataflow representation, used by
hamake is more natural than a workflow.

\section{Future Work}

One of possible hamake improvements is a better integration with
Hadoop schedulers. For example if \textit{Capacity Scheduler} or
\textit{Fair Scheduler} is used, a hamake can take into account
information about its \textit{pools} or \textit{queues} capacities in
job scheduling algorithm.

Another potential area of future extension is hamake dependency
mechanism. Current implementation using fairly simple timestamp
comparison to check if dependency is satisfied. This could be
generalized, allowing user to specify custom dependency check
predicates, implemented either as plugins, scripts in some embedded
scripting language or external programs. This would allow to make
decisions not only based on file meta data (such as timestamp) but
also on its contents.

Several hamake users has requested support of iterative computations
with termination condition. Possible use-cases calling for this type
of usage are fixed-point computations, iterative regression, or
clustering algorithms. Presently, to embed this kind of algorithms
into hamake work flow require use of \textit{generations} feature
combined with external automation, invoking hamake repeatedly until
certain exit condition it satisfied. \textit{hamake} users could
certainly benefit from native support for this kind of dataflows by
hamake.

\nocite{*}
\bibliography{hamake}
\bibliographystyle{unsrt}

\end{document}
